{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SocialDistancingDetection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1CJzcp7-_8xEkWUdC9dmIk5kGdtIkKMC0",
      "authorship_tag": "ABX9TyNf8O+5+miLisQBH04szjTm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mHash1m/My-Python-Experience/blob/master/SocialDistancingDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKqyvG9Xw4Ix"
      },
      "source": [
        "My first Project on OpenCV.\n",
        "\n",
        "This notebook is inspired by this [guide](https://www.pyimagesearch.com/2020/06/01/opencv-social-distancing-detector/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxW74i5kwpFZ"
      },
      "source": [
        "#import required packages\n",
        "import numpy as np\n",
        "import cv2\n",
        "from scipy.spatial import distance as dist\n",
        "import argparse\n",
        "import imutils\n",
        "import os "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmKZzbmrLiqv"
      },
      "source": [
        "#Our Base Path in colab\n",
        "PATH = \"drive/My Drive/Social Distancing Detection\"\n",
        "\n",
        "#input and output files\n",
        "output = os.path.sep.join([PATH, \"output.avi\"])\n",
        "input = os.path.sep.join([PATH, \"videoplayback\"])\n",
        "\n",
        "# # load the COCO class labels our YOLO model was trained on\n",
        "labelsPath = os.path.sep.join([PATH, \"coco-names\"])\n",
        "LABELS = open(labelsPath).read().strip().split(\"\\n\")\n",
        "\n",
        "# # derive the paths to the YOLO weights and model configuration\n",
        "weightsPath = os.path.sep.join([PATH, \"yolov3.weights\"])\n",
        "configPath = os.path.sep.join([PATH, \"yolo.cfg\"])"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZ0_sXf96Jsz"
      },
      "source": [
        "#Initialize Variables for configuration\n",
        "\n",
        "# initialize minimum probability to filter weak detections along with\n",
        "# the threshold when applying non-maxima suppression\n",
        "MIN_CONF = 0.3\n",
        "NMS_THRESH = 0.3\n",
        "\n",
        "# # boolean indicating if NVIDIA CUDA GPU should be used. Kept this False as I didnt wanna get into setting up the GPU requirements on colab.\n",
        "# USE_GPU = False\n",
        "\n",
        "#Min Distance to be observed while social distancing in pixels. Anything above this would be marked as a violation.\n",
        "MIN_DISTANCE = 40\n",
        "\n",
        "display = 0"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzbJ9WCzGljh"
      },
      "source": [
        "Alrighty. We are all set with the variables. Compiling them in a single cell helps experimentation during the rest of the project. Now to get started with our function to use the Yolo model to detect people."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqWOVPNM7Jcz"
      },
      "source": [
        "def detect_people(frame, model, outputL_names, person_idx = 0):\n",
        "  (H, W) = frame.shape[:2]\n",
        "  results = []\n",
        "\n",
        "  boxes = []\n",
        "  centroids = []\n",
        "  confidences = []\n",
        "  #Creating a blob and performing forwards pass.\n",
        "  blob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416),\n",
        "\t\tswapRB=True, crop=False)\n",
        "  model.setInput(blob)\n",
        "  layerOutputs = model.forward(outputL_names)\n",
        "  for output in layerOutputs:\n",
        "    for detection in output:\n",
        "      scores = detection[5:]\n",
        "      classID = np.argmax(scores)\n",
        "      confidence = scores[classID]\n",
        "      if classID == person_idx and confidence > MIN_CONF:\n",
        "        #scale our bounding box according to the frame size.\n",
        "        box = detection[0:4] * np.array([W, H, W, H])\n",
        "        #derive top-left corner\n",
        "        (cX, cY, width, height) = box.astype(\"int\")\n",
        "        blX = int(cX - (width/2))\n",
        "        blY = int(cY - (height/2))\n",
        "\n",
        "        #update our lists\n",
        "        boxes.append([blX, blY, int(width), int(height)])\n",
        "        centroids.append((cX, cY))\n",
        "        confidences.append(float(confidence))\n",
        "\n",
        "  #now lets apply non-maxima suppression\n",
        "  idxs = cv2.dnn.NMSBoxes(boxes, confidences, MIN_CONF, NMS_THRESH)\n",
        "  if len(idxs) > 0:\n",
        "    for idx in idxs.flatten():\n",
        "      (x, y) = (boxes[idx][0], boxes[idx][1])\n",
        "      (w, h) = (boxes[idx][2], boxes[idx][3])\n",
        "      results.append((confidences[idx], (x, y, x+w, y+h), centroids[idx]))\n",
        "  return results"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ws1EXDc9Jgkd"
      },
      "source": [
        "So, we're all set for detecting people and building bounding boxes around the detected persons. All thats left is to use the controids of the detected people and calculate the distance between them(in pixels of course). \n",
        "\n",
        "If the distance exceeds our decided minimum distance to be observed, we mark it as a violation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aesS2uvGkIb",
        "outputId": "fb19f27a-a765-4f44-83eb-bad0ddca3f61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Load our YOLO MODEL trained on COCO dataset from the drive.\n",
        "print(\"Loading our YOLO model ....\")\n",
        "model = cv2.dnn.readNetFromDarknet(configPath, weightsPath)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading our YOLO model ....\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkw4lQVGMyqA",
        "outputId": "6de45e7d-df43-46c3-c727-0b2131eca8dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Determine the only output layer names we need from the YOLO model.\n",
        "outputL_names = model.getLayerNames()\n",
        "outputL_names = [outputL_names[i[0] - 1] for i in model.getUnconnectedOutLayers()]\n",
        "\n",
        "#initialize our video stream and pointer to output file.\n",
        "print(\"Accessing the Video Stream ....\")\n",
        "vs = cv2.VideoCapture(input if input else 0)\n",
        "writer = None"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accessing the Video Stream ....\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYpzf9kzPWIX"
      },
      "source": [
        "#Lets begin reading processing the video frames.\n",
        "\n",
        "while True:\n",
        "  (grabbed, frame) = vs.read()\n",
        "\n",
        "  #break if no frames grabbed\n",
        "  if not grabbed:\n",
        "    break\n",
        "  #scale\n",
        "  frame = imutils.resize(frame, width = 700)\n",
        "\n",
        "  #lets detect some people!\n",
        "  results = detect_people(frame, model, outputL_names, person_idx=LABELS.index(\"person\"))\n",
        "\n",
        "  #We will keep out Violations in a set\n",
        "  violations = set()\n",
        "  v_ = [[] for _ in range(len(results))]\n",
        "  #Start calculating pairwise distances of peple.\n",
        "  if len(results) >= 2:\n",
        "    centroids = np.array([r[2] for r in results])\n",
        "    #calculate Euclidean Distances between pairs of centroids\n",
        "    D = dist.cdist(centroids, centroids, metric=\"euclidean\")\n",
        "    for i in range(0, D.shape[0]):\n",
        "      for j in range(i+1, D.shape[1]):\n",
        "        #check if the distance raises a violation\n",
        "        if D[i, j] < MIN_DISTANCE:\n",
        "          #Add the indexes to our set of violations\n",
        "          violations.add(i)\n",
        "          violations.add(j)\n",
        "          v_[i].append(j)\n",
        "    for (i , (conf, bbox, centroid)) in enumerate(results):\n",
        "      (startX, startY, endX, endY) = bbox\n",
        "      (cX, cY) = centroid\n",
        "      #set the default color of bounding boxes to green.\n",
        "      color = (0 , 255, 0)\n",
        "      #set the violation color of bounding boxes to red.\n",
        "      if i in violations:\n",
        "        color = (0, 0, 255)\n",
        "      #draw bboxes, centroids and lines b/w centroids\n",
        "      cv2.rectangle(frame, (startX, startY), (endX, endY), color, 2)\n",
        "      cv2.circle(frame, (cX, cY), 5, color, 1)\n",
        "      for v in v_[i]:\n",
        "        (_, _, (c2X, c2Y)) = results[v]\n",
        "        cv2.line(frame, (cX, cY), (c2X, c2Y), color, 1)\n",
        "  text = \"Social Distancing Violations: {}\".format(len(violations))\n",
        "  cv2.putText(frame, text, (10, frame.shape[0] - 25), cv2.FONT_HERSHEY_SIMPLEX, 0.85, (0, 0, 255) if (len(violations) > 0) else (0, 255, 0), 3)\n",
        "\n",
        "  # check to see if the output frame should be displayed to our\n",
        "  # screen\n",
        "  if display > 0:\n",
        "    # show the output frame\n",
        "    cv2.imshow(\"Frame\", frame)\n",
        "    key = cv2.waitKey(1) & 0xFF\n",
        "    # if the `q` key was pressed, break from the loop\n",
        "    if key == ord(\"q\"):\n",
        "      break\n",
        "  # if an output video file path has been supplied and the video\n",
        "  # writer has not been initialized, do so now\n",
        "  if output != \"\" and writer is None:\n",
        "    # initialize our video writer\n",
        "    fourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
        "    writer = cv2.VideoWriter(output, fourcc, 25,\n",
        "      (frame.shape[1], frame.shape[0]), True)\n",
        "  # if the video writer is not None, write the frame to the output\n",
        "  # video file\n",
        "  if writer is not None:\n",
        "    writer.write(frame)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ko98R38AR9ed"
      },
      "source": [
        ""
      ],
      "execution_count": 7,
      "outputs": []
    }
  ]
}